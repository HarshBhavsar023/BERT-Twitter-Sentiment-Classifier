{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Twitter Sentiment Classifier .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMF0aMV48a8RvSKikiHEVNl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshBhavsar023/BERT-Twitter-Sentiment-Classifier/blob/main/BERT_Twitter_Sentiment_Classifier_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlmzwT6bjN-8"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
        "from sklearn.metrics import accuracy_score,matthews_corrcoef\n",
        "\n",
        "from tqdm import tqdm, trange,tnrange,tqdm_notebook\n",
        "import random\n",
        "import os\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx7uskcYjUcg"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "\n",
        "SEED = 19\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == torch.device(\"cuda\"):\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9RRzNC1jcWX"
      },
      "source": [
        "### Identify and specify the GPU as the device, later in training loop we will load data into device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nORMl_5jqyg"
      },
      "source": [
        "df_train = pd.read_csv(\"/kaggle/input/twitter-sentiment-dataset/Twitter_Data.csv\")\n",
        "df_train.isnull().sum()\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6SDue0Sj5aF"
      },
      "source": [
        "Target distributuion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmILx_3XjukE"
      },
      "source": [
        "df_train['category'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2O666pRkClx"
      },
      "source": [
        "df_train['category'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VidWwVe1kHsO"
      },
      "source": [
        "Data cleaning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbnBXlR8kF0p"
      },
      "source": [
        "# Ignoring null values \n",
        "df_train = df_train[~df_train['category'].isnull()]\n",
        "df_train = df_train[~df_train['clean_text'].isnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5OIYpUOkVkc"
      },
      "source": [
        "Target encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He5AKAH-kRC4"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "df_train['category_1'] = labelencoder.fit_transform(df_train['category'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcU_R_BWkaXA"
      },
      "source": [
        "df_train[['category','category_1']].drop_duplicates(keep='first')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7-j5rXOkf_F"
      },
      "source": [
        "df_train.rename(columns={'category_1':'label'},inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9HYiDm4kk8g"
      },
      "source": [
        "\n",
        "Data Preperation for BERT model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kvHlr-Fkobx"
      },
      "source": [
        "## create label and sentence list\n",
        "sentences = df_train.clean_text.values\n",
        "\n",
        "#check distribution of data based on labels\n",
        "print(\"Distribution of data based on labels: \",df_train.label.value_counts())\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 256\n",
        "\n",
        "## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyPj3-Lbkt5A"
      },
      "source": [
        "input_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True,truncation=True) for sent in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlnxlD7nkxoq"
      },
      "source": [
        "labels = df_train.label.values\n",
        "\n",
        "print(\"Actual sentence before tokenization: \",sentences[2])\n",
        "print(\"Encoded Input from dataset: \",input_ids[2])\n",
        "\n",
        "## Create attention mask\n",
        "attention_masks = []\n",
        "## Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
        "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
        "print(attention_masks[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYgPMoBdk3I2"
      },
      "source": [
        "train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)\n",
        "train_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3fy5hEKlB1V"
      },
      "source": [
        "# convert all our data into torch tensors, required data type for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "train_data = TensorDataset(train_inputs,train_masks,train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\n",
        "validation_sampler = RandomSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdPjGeQblMy9"
      },
      "source": [
        "Load BERT For Sequence Classification, the pretrained BERT model with a single linear classification layer on top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX4FXTCnlT5x"
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3).to(device)\n",
        "\n",
        "# Parameters:\n",
        "lr = 2e-5\n",
        "adam_epsilon = 1e-8\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "num_warmup_steps = 0\n",
        "num_training_steps = len(train_dataloader)*epochs\n",
        "\n",
        "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
        "optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU0efZFTleob"
      },
      "source": [
        "Traning and Interface "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po99rqC9lg3S"
      },
      "source": [
        "## Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "learning_rate = []\n",
        "\n",
        "# Gradients gets accumulated by default\n",
        "model.zero_grad()\n",
        "\n",
        "# tnrange is a tqdm wrapper around the normal python range\n",
        "for _ in tnrange(1,epochs+1,desc='Epoch'):\n",
        "  print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
        "  # Calculate total loss for this epoch\n",
        "  batch_loss = 0\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "    \n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    loss = outputs[0]\n",
        "    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Clip the norm of the gradients to 1.0\n",
        "    # Gradient clipping is not in AdamW anymore\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    \n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Update learning rate schedule\n",
        "    scheduler.step()\n",
        "\n",
        "    # Clear the previous accumulated gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Update tracking variables\n",
        "    batch_loss += loss.item()\n",
        "\n",
        "  # Calculate the average loss over the training data.\n",
        "  avg_train_loss = batch_loss / len(train_dataloader)\n",
        "\n",
        "  #store the current learning rate\n",
        "  for param_group in optimizer.param_groups:\n",
        "    print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
        "    learning_rate.append(param_group['lr'])\n",
        "    \n",
        "  train_loss_set.append(avg_train_loss)\n",
        "  print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits[0].to('cpu').numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "    labels_flat = label_ids.flatten()\n",
        "    \n",
        "    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\n",
        "    \n",
        "    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n",
        "    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    eval_mcc_accuracy += tmp_eval_mcc_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n",
        "  print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDjWDy9wlquZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    import itertools\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgCGq1qsl0ES"
      },
      "source": [
        "## emotion labels\n",
        "label2int = {\n",
        "  \"Negative\": 0,\n",
        "  \"Neutral\": 1,\n",
        "  \"Positive\": 2\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpY1N3X7l3pO"
      },
      "source": [
        "print(classification_report(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values, target_names=label2int.keys(), digits=len(label2int)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oz4mKbZl_ih"
      },
      "source": [
        "model_save_folder = 'model/'\n",
        "tokenizer_save_folder = 'tokenizer/'\n",
        "\n",
        "path_model = F'/kaggle/working/{model_save_folder}'\n",
        "path_tokenizer = F'/kaggle/working/{tokenizer_save_folder}'\n",
        "\n",
        "##create the dir\n",
        "\n",
        "!mkdir -p {path_model}\n",
        "!mkdir -p {path_tokenizer}\n",
        "\n",
        "### Now let's save our model and tokenizer to a directory\n",
        "model.save_pretrained(path_model)\n",
        "tokenizer.save_pretrained(path_tokenizer)\n",
        "\n",
        "model_save_name = 'fineTuneModel.pt'\n",
        "path = path_model = F'/kaggle/working/{model_save_folder}/{model_save_name}'\n",
        "torch.save(model.state_dict(),path);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWNriBXBmLEM"
      },
      "source": [
        "###Conclusion\n",
        "- With Transfer learning approach , We are using pretrained BERT model to classify tweets in the dataset with Negative , Neutral and Positive , Hope you find this kernal as useful"
      ]
    }
  ]
}